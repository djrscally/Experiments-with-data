{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep Neural Networks with Keras\n",
    "## For general machine-learning problems\n",
    "\n",
    "#### Summary\n",
    "\n",
    "In their paper [Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/1606.07792.pdf), Google outlines a neural network architecture that combines the benefits of two different models. Under a wide model, an extremely sparse matrix consisting primarily of cross-product transformations of features is linked directly to an output neuron. Under a deep model, the inputs are passed through Embedding layers, followed by consecutive densely connected hidden layers before finally arriving at the output. Under the combined Wide & Deep Model the benefits of both systems (good memorization and generalization respectively) are retained by combining the outputs and training the models consecutively, in this form:\n",
    "\n",
    "![wad](wad.PNG)\n",
    "\n",
    "This notebook demonstrates the building of a wide-and-deep model architecture in Keras and its use in making predictions against the [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) dataset from UCIMLR. Note that this is not necessarily the best method, and certainly not the easiest. Google in fact provides a pre-made classifier as part of TensorFlow called the [DNNLinearCombinedClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier) which should be your first port of call if you want to use such a technique. The intent in doing this so manually was simply to solidify my understanding of the method, and I hope that it can do the same for others too.\n",
    "\n",
    "Let's delve straight in with the imports. I'm going to be using the functional API for Keras, so that's what I've imported. I'm also instantiating the dataset, and then outputting a `.head()`. to demonstrate the look of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Flatten, Reshape, Concatenate\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./presentation/adult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select_dtypes('object').to_csv(r'C:\\dev\\tb\\metadata.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      "age               32561 non-null int64\n",
      "workclass         32561 non-null object\n",
      "fnlwgt            32561 non-null int64\n",
      "education         32561 non-null object\n",
      "education-num     32561 non-null int64\n",
      "marital-status    32561 non-null object\n",
      "occupation        32561 non-null object\n",
      "relationship      32561 non-null object\n",
      "race              32561 non-null object\n",
      "sex               32561 non-null object\n",
      "capital-gain      32561 non-null int64\n",
      "capital-loss      32561 non-null int64\n",
      "hours-per-week    32561 non-null int64\n",
      "native-country    32561 non-null object\n",
      "income            32561 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  income  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is the 'income' column. We can see we have a number of continuous and categorical features, and we'll deal with these in different ways.\n",
    "\n",
    "#### The Deep Model\n",
    "\n",
    "This will comprise the continuous features in unaltered form, since they're already in an appropriate format. It will also include all of the categorical features, which will each be passed through its own [Embedding](https://www.tensorflow.org/guide/embedding) layer. I won't go into Embedding too much (follow the link to learn more), but suffice to say it's essentially a means of encoding each categorical feature into $N$ new features of floating point values. $N$ is a number chosen by the user, and the actual values that represent each category are learned as part of the training process, just like the weights for Dense layers. In my case I'm copying Google's example and setting $N$ to 32. There are 8 categorical features, plus 6 continuous ones, which means the dense model will have 262 inputs in total.\n",
    "\n",
    "#### The Wide Model\n",
    "\n",
    "This will comprise two things;\n",
    "\n",
    "1. Each of the 8 categorical features, one-hot encoded.\n",
    "2. A cross product transformation of each of the features created in step 1. \n",
    "\n",
    "The cross-product is where things become especially interesting. This effectively creates new binary features in the form `and(featureA, featureB)`, `and(featureA, featureC)`, `and(featureB, featureC)` and so on. Remember though that the features here are not the original \"workclass\" / \"occupation\" etc but rather their one-hot encoded children, which means the features created will be of the form `and(workclass_State-gov, occupation_Adm-clerical)` and `and(workclass_Private, occupation_Handlers-cleaners)` and so on. As you can imagine; this makes for an extremely large and sparse matrix, some 10,500 new features in our case. \n",
    "\n",
    "We'll first define which columns fall into which bracket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "deep_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "wide_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then define a function that one-hot encodes the wide columns and subsequently performs the cross-product transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 10506)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_wide_features(df, wide_cols):\n",
    "    for col in wide_cols:\n",
    "        df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "        \n",
    "    for col in wide_cols:\n",
    "        for next_col in np.delete(wide_cols, np.argwhere(wide_cols==col)):\n",
    "            for col_val in df[col].unique():\n",
    "                for next_col_val in df[next_col].unique():\n",
    "                    df[col + '_' + col_val + '_x_' + next_col + '_' + next_col_val] = df[col + '_' + col_val] & df[next_col + '_' + next_col_val]\n",
    "                    \n",
    "    df.drop(wide_cols, axis=1, inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "wide = build_wide_features(\n",
    "    df=df.drop(\n",
    "        [\n",
    "            'age',\n",
    "            'fnlwgt',\n",
    "            'education-num',\n",
    "            'capital-gain',\n",
    "            'capital-loss',\n",
    "            'hours-per-week',\n",
    "            'income'\n",
    "        ], axis=1),\n",
    "    wide_cols=wide_cols\n",
    ")\n",
    "\n",
    "wide.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10,506 features. Oof. Next step is to do some preprocessing on the dataframe. We will...\n",
    "\n",
    "1. Label-Encode the categorical columns; a necessary precursor to the Embedding\n",
    "2. Strip out the target feature\n",
    "3. Concatenate on our 10,506 new wide features so everything's in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_x_cols = wide.columns\n",
    "\n",
    "for col in deep_cols:\n",
    "    df[col] = df[col].astype('category').cat.codes\n",
    "\n",
    "y = df.income.astype('category').cat.codes\n",
    "df = pd.concat([df, wide], axis=1).drop('income', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the actual magic, the definition of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the easy-peasy Input for our continuous features. We have 6 numeric features,\n",
    "# so we just instantiate an Input of shape 6.\n",
    "continuous_inputs = Input(shape=(6,), name='continuous-inputs')\n",
    "\n",
    "# Next, we create an Input - Embed - Flatten system for each of the  categorical deep columns, and then merge them together\n",
    "deep_inputs = []\n",
    "deep_outputs = []\n",
    "\n",
    "for col in deep_cols:\n",
    "    i = Input(shape=(1,), name=col+'-inputs')\n",
    "    deep_inputs.append(i)\n",
    "    e = Embedding(df[col].nunique()+1, 32, input_length=1, name=col+'-embedding')(i)\n",
    "    f = Flatten(name=col+'-flatten')(e)\n",
    "    deep_outputs.append(f)\n",
    "    \n",
    "deep_merge = Concatenate(name='deep-merge')([continuous_inputs] + deep_outputs)\n",
    "\n",
    "# Then add the three deep layers\n",
    "\n",
    "d1 = Dense(1024, activation='tanh', name='ReLU-1024')(deep_merge)\n",
    "d2 = Dense(512, activation='relu', name='ReLU-512')(d1)\n",
    "d3 = Dense(128, activation='relu', name='ReLU-256')(d2)\n",
    "\n",
    "# Next we add the wide bit; a simple input matching the 10,506 dimensions of that\n",
    "# part of our dataset plus an output layer with a single neuron.\n",
    "\n",
    "wide_input = Input(shape=(wide.shape[1],), name='wide-inputs')\n",
    "\n",
    "# And merge the two segments outputs into one unified output\n",
    "wad_merge = Concatenate(name='wide-and-deep-merge')([d3, wide_input])\n",
    "wado = Dense(1, activation='sigmoid', name='wide-and-deep-out')(wad_merge)\n",
    "\n",
    "# And finally, compile the model\n",
    "model = Model(inputs=[continuous_inputs] + deep_inputs + [wide_input],outputs=wado)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print a summary to take a look at the structure we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "workclass-inputs (InputLayer)   (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "education-inputs (InputLayer)   (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "marital-status-inputs (InputLay (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "occupation-inputs (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "relationship-inputs (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "race-inputs (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sex-inputs (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "native-country-inputs (InputLay (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "workclass-embedding (Embedding) (None, 1, 32)        320         workclass-inputs[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "education-embedding (Embedding) (None, 1, 32)        544         education-inputs[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "marital-status-embedding (Embed (None, 1, 32)        256         marital-status-inputs[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "occupation-embedding (Embedding (None, 1, 32)        512         occupation-inputs[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "relationship-embedding (Embeddi (None, 1, 32)        224         relationship-inputs[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "race-embedding (Embedding)      (None, 1, 32)        192         race-inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sex-embedding (Embedding)       (None, 1, 32)        96          sex-inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "native-country-embedding (Embed (None, 1, 32)        1376        native-country-inputs[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "continuous-inputs (InputLayer)  (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "workclass-flatten (Flatten)     (None, 32)           0           workclass-embedding[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "education-flatten (Flatten)     (None, 32)           0           education-embedding[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "marital-status-flatten (Flatten (None, 32)           0           marital-status-embedding[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "occupation-flatten (Flatten)    (None, 32)           0           occupation-embedding[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relationship-flatten (Flatten)  (None, 32)           0           relationship-embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "race-flatten (Flatten)          (None, 32)           0           race-embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sex-flatten (Flatten)           (None, 32)           0           sex-embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "native-country-flatten (Flatten (None, 32)           0           native-country-embedding[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "deep-merge (Concatenate)        (None, 262)          0           continuous-inputs[0][0]          \n",
      "                                                                 workclass-flatten[0][0]          \n",
      "                                                                 education-flatten[0][0]          \n",
      "                                                                 marital-status-flatten[0][0]     \n",
      "                                                                 occupation-flatten[0][0]         \n",
      "                                                                 relationship-flatten[0][0]       \n",
      "                                                                 race-flatten[0][0]               \n",
      "                                                                 sex-flatten[0][0]                \n",
      "                                                                 native-country-flatten[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "deep-1024 (Dense)               (None, 1024)         269312      deep-merge[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "deep-512 (Dense)                (None, 512)          524800      deep-1024[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "deep-256 (Dense)                (None, 128)          65664       deep-512[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "wide-inputs (InputLayer)        (None, 10506)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "wide-and-deep-merge (Concatenat (None, 10634)        0           deep-256[0][0]                   \n",
      "                                                                 wide-inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "wide-and-deep-out (Dense)       (None, 1)            10635       wide-and-deep-merge[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 873,931\n",
      "Trainable params: 873,931\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yeah, that's looking good. It's also useful to take a look at a plot of the model, which we can optain with `plot_model(model, model.png)`:\n",
    "\n",
    "![plot](model.png)\n",
    "\n",
    "And if we compare that to the image from Google's paper about their final model architecture, you can see it's an easy match:\n",
    "\n",
    "![gplot](google_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the training process; 25% of dataset split as validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24420 samples, validate on 8141 samples\n",
      "Epoch 1/50\n",
      "24420/24420 [==============================] - 28s 1ms/step - loss: 0.3734 - acc: 0.8257 - val_loss: 0.3602 - val_acc: 0.8284\n",
      "Epoch 2/50\n",
      "24420/24420 [==============================] - 31s 1ms/step - loss: 0.3515 - acc: 0.8356 - val_loss: 0.3531 - val_acc: 0.8337\n",
      "Epoch 3/50\n",
      "24420/24420 [==============================] - 29s 1ms/step - loss: 0.3483 - acc: 0.8373 - val_loss: 0.3546 - val_acc: 0.8336\n",
      "Epoch 4/50\n",
      "24420/24420 [==============================] - 27s 1ms/step - loss: 0.3442 - acc: 0.8388 - val_loss: 0.3554 - val_acc: 0.8338\n",
      "Epoch 5/50\n",
      "24420/24420 [==============================] - 27s 1ms/step - loss: 0.3408 - acc: 0.8415 - val_loss: 0.3553 - val_acc: 0.8318\n",
      "Epoch 6/50\n",
      "24420/24420 [==============================] - 27s 1ms/step - loss: 0.3393 - acc: 0.8413 - val_loss: 0.3590 - val_acc: 0.8321\n",
      "Epoch 7/50\n",
      "24420/24420 [==============================] - 26s 1ms/step - loss: 0.3381 - acc: 0.8412 - val_loss: 0.3587 - val_acc: 0.8313\n",
      "Epoch 8/50\n",
      "24420/24420 [==============================] - 33s 1ms/step - loss: 0.3360 - acc: 0.8427 - val_loss: 0.3616 - val_acc: 0.8317\n",
      "Epoch 9/50\n",
      "24420/24420 [==============================] - 37s 1ms/step - loss: 0.3356 - acc: 0.8418 - val_loss: 0.3620 - val_acc: 0.8304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc00d48fd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer_names = ['workclass-embedding', 'education-embedding', 'marital-status-embedding', 'occupation-embedding', 'relationship-embedding', 'race-embedding', 'sex-embedding', 'native-country-embedding']\n",
    "\n",
    "es = EarlyStopping(monitor='val_acc', mode='max', patience=5, restore_best_weights=True)\n",
    "tb = TensorBoard(log_dir=r'C:\\dev\\tb\\wad', histogram_freq=5, write_grads=True, embeddings_freq=5, embeddings_layer_names=embedding_layer_names, embeddings_data=[df[num_cols]] + [df[[i]] for i in deep_cols] + [df[wide_x_cols]]) \n",
    "model.fit([df[num_cols]] + [df[[i]] for i in deep_cols] + [df[wide_x_cols]], y, batch_size=32, epochs=50, validation_split=0.25, callbacks=[es, tb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For shiggles, let's look at the deep model only and the wide model only too. Deep only requries we add a new output layer and route d3 into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24420 samples, validate on 8141 samples\n",
      "Epoch 1/5\n",
      "24420/24420 [==============================] - 9s 363us/step - loss: 0.5494 - acc: 0.7578 - val_loss: 0.5399 - val_acc: 0.7643\n",
      "Epoch 2/5\n",
      "24420/24420 [==============================] - 6s 246us/step - loss: 0.5365 - acc: 0.7695 - val_loss: 0.5314 - val_acc: 0.7667\n",
      "Epoch 3/5\n",
      "24420/24420 [==============================] - 6s 229us/step - loss: 0.5361 - acc: 0.7724 - val_loss: 0.5681 - val_acc: 0.7619\n",
      "Epoch 4/5\n",
      "24420/24420 [==============================] - 6s 255us/step - loss: 0.5451 - acc: 0.7661 - val_loss: 0.5479 - val_acc: 0.7619\n",
      "Epoch 5/5\n",
      "24420/24420 [==============================] - 8s 339us/step - loss: 0.5440 - acc: 0.7659 - val_loss: 0.5480 - val_acc: 0.7616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc5562e240>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do = Dense(1, activation='sigmoid', name='deep-only-output')(d3)\n",
    "deep_only = Model(inputs=[continuous_inputs] + deep_inputs, outputs=do)\n",
    "deep_only.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "deep_only.fit([df[num_cols]] + [df[[i]] for i in deep_cols], y, batch_size=128, epochs=5, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, wide only just requires a new output layer with the wide inputs routed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dscally\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"wi..., outputs=Tensor(\"wi...)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24420 samples, validate on 8141 samples\n",
      "Epoch 1/5\n",
      "24420/24420 [==============================] - 6s 260us/step - loss: 0.4788 - acc: 0.8244 - val_loss: 0.4709 - val_acc: 0.8241\n",
      "Epoch 2/5\n",
      "24420/24420 [==============================] - 5s 212us/step - loss: 0.4636 - acc: 0.8244 - val_loss: 0.5113 - val_acc: 0.8173\n",
      "Epoch 3/5\n",
      "24420/24420 [==============================] - 5s 201us/step - loss: 0.4412 - acc: 0.8310 - val_loss: 0.4925 - val_acc: 0.8231\n",
      "Epoch 4/5\n",
      "24420/24420 [==============================] - 7s 271us/step - loss: 0.4364 - acc: 0.8248 - val_loss: 0.5505 - val_acc: 0.8100\n",
      "Epoch 5/5\n",
      "24420/24420 [==============================] - 6s 229us/step - loss: 0.4364 - acc: 0.8330 - val_loss: 0.4887 - val_acc: 0.8231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc5874aeb8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wo = Dense(1, activation='relu', name='wide-out')(wide_input)\n",
    "wide_only = Model(inputs=wide_input, output=wo)\n",
    "wide_only.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "wide_only.fit(df[wide_x_cols], y, batch_size=128, epochs=5, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the wide and deep model performs better than either of its components alone. Let's do a quick comparison to sklearn's LogisticRegression implementation as an extra benchmark. I'd use a more sophisticated algorithm like RandomForestClassifier; but I 'aint waiting for it to run through 10,507 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dscally\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "c:\\users\\dscally\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\dscally\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\dscally\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "scores = cross_validate(\n",
    "    lr,\n",
    "    df,\n",
    "    y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8212893503934996"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse than the wide and deep too. Although not by much! So there you have it; a wide and deep model implemented in keras. Hope this helps your understanding of this particular model too; if you have any questions feel free to drop in an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
